\documentclass{article}
\usepackage{amsmath}

\title{Chapter theory 2 solution}
\date{2016-08-27}
\author{Artem Puzanov}


\begin{document}

\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}

\section{Matrix representations of main equations}
\subsection{part I}
\paragraph{Intuition}
Hadamard product is just an easy way to use vectors instead of diagonal matrices, not much thinking needed here )
\paragraph{Mathematical notation}
To prove this assertion, we must show that 
$$\delta^L = {\sum}'(z^L)\nabla_aC = \nabla_aC\odot\sigma'(z^L)$$
Let's call $\delta^L$ for non-Hadamard case $\delta'^L$.
Or equivalently, we must prove that $$\forall i: \delta^L_i = \delta'^L_i$$,
where $$\delta^L_i = \sigma'(z_i^L)\frac{\partial C}{\partial a_i}$$
Let's write $\delta'^L_i$ in index hell notation. 
A small note: I shall use $I^{\delta}$ as replacement for $\sum$ notation, as I shall use $\sum$ in it's usual arithmetical sum meaning further.
$$\delta'^L_j = {\sum}_{j}I^{\delta}_{ij}\frac{\partial C}{\partial a_j}$$
By definition, $I^{\delta}_{i\neqj} = 0$, therefore:
$${\sum}_{j}I^{\delta}_{ij}\frac{\partial C}{\partial a_j} = I^{\delta}_{ii} \frac{\partial C}{\partial a_i}$$
Replacing $I^{\delta}_{ii}$ with $\sigma'(z_i^L)$ we get 
$$\delta'^L_i = \sigma'(z_i^L)\frac{\partial C}{\partial a_i}$$
Which is what we wanted to prove;

\subsection{part II}
\paragraph{Intuition}
Again, nothing hard here. Just prove for any $j$ that result is equivalent to Hadamar project
\paragraph{Mathematical notation}

Let's call $\delta^l$ for non-Hadamard case $\delta'^l$.
To prove the statement, we must show that 
$$\delta^l = {\sum}'(z^l) (w^{l+1})^{T} \delta^{l+1} = (w^{l+1})^{T} \delta^{l+1}\odot\sigma'(z^l)$$

Or equivalently, we must prove that $$\forall i: \delta^l_i = \delta'^l_i$$, where 
$$\delta^l_i = \sigma'(z_i^l) (\sum_j(w^{l+1})^T_{ij} \delta^{l+1}_j)_i$$

A small note: I shall use $I^{\delta}$ as replacement for $\sum$ notation, as I shall use $\sum$ in it's usual arithmetical sum meaning further.

For non-hadamar case
$$\delta'^l_i = \sum_k I^{\delta}_{ik} (\sum_j(w^{l+1})^T_{ij} \delta^{l+1}_j)_k$$
By definition, $I^{\delta}_{i \neq k} = 0$, therefore:
$$\delta'^l_i = I^{\delta}_{ii} (\sum_j(w^{l+1})^T_{ij} \delta^{l+1}_j)_i$$
Replacing $I^{\delta}_{ii}$ with $\sigma'(z_i^l)$ we get 
$$\delta'^l_i = \sigma'(z_i^l) (\sum_j(w^{l+1})^T_{ij} \delta^{l+1}_j)_i$$
q.e.d.

\subsection{part III}
\paragraph{Intuition}
Not really sure what to show here. Just insert both statements by induction form L to l.


\section{Proof of fundamental equations (3) and (4)}
\subsection{Intuition, equation 3}
As stated in the chapter, these equations are just result of derivative-taking rules.
Essentialy you have to go $into$ $C$, than $into$ $a$, than into $z$, where $C$ is loss function of value of $\sigma$, $\sigma$ is activation funtion of $z$, and $z$ is function of $w$ and $b$
\subsection{Mathematical notation, equation 3}
The functional form of cost function is $C(\sigma(z(w,b)))$.
We have to prove that 
$$\frac{\partial C}{\partial b_i} = \delta^l_i$$
Using chain rule:
$$\frac{\partial C}{\partial b_j} = \frac{\partial C}{\partial \sigma_j} \frac{\partial \sigma_j}{\partial z_j} \frac{\partial z_j}{\partial b_j}$$
$$z_j = \sum_jw_ja_j^{l-1} + b_j$$
$$\frac{\partial z_j}{\partial b_j} = 1$$
than 
$$\frac{\partial C}{\partial b_j} = \frac{\partial C}{\partial \sigma_j} \frac{\partial \sigma_j}{\partial z^l_j}$$
We can notice that by definition of the chain rule (given that $\sigma$ is a function of $z$):
$$\frac{\partial C}{\partial \sigma_j} \frac{\partial \sigma_j}{\partial z^l_j} = \frac{\partial C}{\partial z^l_j}$$
Which is precisely the definition of $\delta^l_i$ (equation number 29 in the book)

\subsection{Intuition, equation 4}
We shall use the same trick here, except we have more derivative complexity in $z^l_j$
\subsection{Mathematical notation, equation 4}
The functional form of cost function is $C(\sigma(z(w,b)))$.
We have to prove that 
$$\frac{\partial C}{\partial w^l_{ij}} = \delta^l_i a^{l-1}_j$$
Using chain rule:
$$\frac{\partial C}{\partial w_{ij}} = \frac{\partial C}{\partial \sigma_j} \frac{\partial \sigma_j}{\partial z_j} \frac{\partial z_j}{\partial w_{ij}}$$
By (INSERT NUMBER FROM THE BOOK HERE)
$$z_i = \sum_jw_{ij}a^{l-1}_j + b_i$$
So
$$\frac{\partial z_j}{\partial w_{ij}} = a^{l-1}_j$$
If we put that into our chain rule equation we get:
$$\frac{\partial C}{\partial w_{ij}} = \frac{\partial C}{\partial \sigma_j} \frac{\partial \sigma_j}{\partial z_j} a^{l-1}_j$$

We can notice that by definition of the chain rule (given that $\sigma$ is a function of $z$):
$$\frac{\partial C}{\partial \sigma_j} \frac{\partial \sigma_j}{\partial z^l_j} = \frac{\partial C}{\partial z^l_j}$$
Which is precisely the definition of $\delta^l_i$ (equation number 29 in the book)
Putting that into our previous equations we get:
$$\frac{\partial C}{\partial w_{ij}} = \delta^l_i a^{l-1}_j$$
Which is exactly (BP4)

\section{Backpropagation algorithm adaptation}
\subsection{Intuition, custom activation function}
Every neuron is pretty lonely and isolated (too primitive to understand that though), so we just have to replace his personal derivatives, he won't notice.
\subsection{Mathematical notation, custom activation function}
Let's start with BP1 for arbitrary neuron $i$:
$$\delta^l_i = \frac{\partial C}{\partial \sigma_j} \sigma'(z^l_i)$$
where $\sigma$ is an activation function. Replacing that with arbitrarty $f$ gives us
$$\delta^l_i = \frac{\partial C}{\partial f_i} f'(z^l_i)$$
Now for BP2:
$$\delta^l_i = \sigma'(z_i^l) (\sum_j(w^{l+1})^T_{ij} \delta^{l+1}_j)_i$$
again, we can replace that with arbitrary $f$:
$$\delta^l_i = f'(z_i^l) (\sum_j(w^{l+1})^T_{ij} \delta^{l+1}_j)_i$$

Those two were easy. Now for interesting cases. Let's try $BP3$ first. By applying chain rule
$$\frac{\partial C}{\partial b_j} = \frac{\partial C}{\partial \sigma_j} \frac{\partial \sigma_j}{\partial b_j}$$
The first multiplier is easy - it's either result of modified BP1 or modified BP2. We'll call it $\delta_j'$.
The second one is just $\frac{\partial f_j}{\partial b_j}$. We don't need to use chain rule further, as $b_j$ is a constant.
Replacing both multipliers we get:
$$\frac{\partial C}{\partial b_j} = \delta_j' \frac{\partial f_j}{\partial b_j}$$

Now let's go for BP4:
$$\frac{\partial C}{\partial w_{ij}} = \delta_i a^{l-1}_{j}$$
Let's rewrite it using chain rule
$$\frac{\partial C}{\partial w_{ij}} = \frac{\partial C}{\partial \sigma_j} \frac{\partial \sigma_j}{\partial w_{ij}}$$
Using $\delta_j'$ replacement from BP3 solution, and replacing $\sigma$ with $f$ in the second mulitplier:
$$\frac{\partial C}{\partial w_{ij}} = \delta_j' \frac{\partial f_j}{\partial w_{ij}}$$
We can further expand this eqation by applying chain rule to $\frac{\partial f_j}{\partial w_{ij}}$:
$$\frac{\partial C}{\partial w_{ij}} = \delta_j' \frac{\partial f_j}{\partial z_{ij}} a^{l-1}_{j}$$

This concludes equations for arbitrary $f$ instead of $\sigma$ activation function

\subsection{Intuition, linear activation function}
We can use our previous exercise result to specify BP 1-4 for linear case, by substitution of linear $f(z)$ for arbitrary $f(z)$.
\subsection{Mathematical notation, linear activation function}
In all our eqautions we simply substitue $f'(z^l_i) = 1$:

CHECK INDEXES! ${ij}$ THING IS LIKELY VERY VERY WRONG

BP1:
$$\delta^l_i = \frac{\partial C}{\partial f_i} f'(z^l_i) = \frac{\partial C}{\partial f_i}  1$$
BP2:
$$\delta^l_i = f'(z_i^l) (\sum_j(w^{l+1})^T_{ij} \delta^{l+1}_j)_i = 1  (\sum_j(w^{l+1})^T_{ij} \delta^{l+1}_j)_i$$
BP3:
$$\frac{\partial C}{\partial b_j} = \delta_j' \frac{\partial f_j}{\partial b_j} = \delta_j'  1$$
BP4:
$$\frac{\partial C}{\partial w_{ij}} = \delta_j'  1  a^{l-1}_{j}$$
This concludes modification of backpropagation equations

\end{document}
