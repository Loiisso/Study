\documentclass{article}
\usepackage{amsmath}

\title{Chapter theory 3 solution}
\date{2016-09-27}
\author{Artem Puzanov}


\begin{document}

\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}

\section{activation derivative proof}
\paragraph{Intuition}
Taking derivatives from exponents rather often ends in a kind of semi-recursive definitions, which come in handy for computational purposes.
\paragraph{Mathematical notation}
We want to prove that:
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
where $\sigma(z) = \frac{1}{1+e^{-z}}$
This is a rather simple derivation, using chain rule:
$$\sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2}$$
Let's take out $\sigma(z)$
$$\sigma'(z) = \frac{1}{(1 + e^{-z})}\frac{e^{-z}}{1+e^{-z}}$$
$$\sigma'(z) = \sigma(z)\frac{e^{-z} + 1 - 1}{1+e^{-z}}$$
$$\sigma'(z) = \sigma(z)(1 - \frac{1}{1+e^{-z}})$$
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
Q.E.D.

\section{entropy function exercies}
\subsection{entropy form memorization}
\paragraph{Intuition}
It's easy to remember specific form, if you follow two rules: it's symmetrical, and logariphm is not defined at zero.
Symmetricity gives: $xln(y) + (1-x)ln(1-y)$, and whether $x$ or $y$ is $a$ or $y$ is easy to decide, if we remember that
true value can take values in $[0,1]$ interval, and activation can't reach $0$ or $1$. 
This means that only activation value can be under $ln$.
No mathematical proof needed here.
\subsection{proof of entropy minimization for regression-like problems}
\paragraph{Intuition}
Nothing especially hard here, usual function-minimization proof. 
Take the first derivative, solve it relative to zero, show that this is minimizing solution.
\paragraph{Mathematical notation}
First let's show that $y = a$ is a solution for:
 $$\frac{\partial C}{\partial a} = 0$$
 $$\frac{\partial C}{\partial a} = -\frac{1}{n}\sum(\frac{y}{a} - \frac{1-y}{1-a}) = 0$$
Let's drop $-\frac{1}{n}$, and concetrate on the expression under sum:
 $$\frac{y}{a} - \frac{1-y}{1-a} = 0$$
 $$\frac{y}{a} = \frac{1-y}{1-a}$$
 $$y(1-a) = (1-y)a$$
$$a = y$$
As neurons in the final layer are not intersecting, this holds true for all $j$

\end{document}