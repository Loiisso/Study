\documentclass{article}
\usepackage{amsmath}

\title{Chapter theory 3 solution}
\date{2016-09-27}
\author{Artem Puzanov}


\begin{document}

\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}

A note: this chapter is gargantuan in size, so A LOT of exercises and problems. Not my fault:)

\section{activation derivative proof}
\paragraph{Intuition}
Taking derivatives from exponents rather often ends in a kind of semi-recursive definitions, which come in handy for computational purposes.
\paragraph{Mathematical notation}
We want to prove that:
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
where $\sigma(z) = \frac{1}{1+e^{-z}}$
This is a rather simple derivation, using chain rule:
$$\sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2}$$
Let's take out $\sigma(z)$
$$\sigma'(z) = \frac{1}{(1 + e^{-z})}\frac{e^{-z}}{1+e^{-z}}$$
$$\sigma'(z) = \sigma(z)\frac{e^{-z} + 1 - 1}{1+e^{-z}}$$
$$\sigma'(z) = \sigma(z)(1 - \frac{1}{1+e^{-z}})$$
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
Q.E.D.

\section{entropy function exercies}
\subsection{entropy form memorization}
\paragraph{Intuition}
It's easy to remember specific form, if you follow two rules: it's symmetrical, and logariphm is not defined at zero.
Symmetricity gives: $xln(y) + (1-x)ln(1-y)$, and whether $x$ or $y$ is $a$ or $y$ is easy to decide, if we remember that
true value can take values in $[0,1]$ interval, and activation can't reach $0$ or $1$. 
This means that only activation value can be under $ln$.
No mathematical proof needed here.
\subsection{proof of entropy minimization for regression-like problems}
\paragraph{Intuition}
Nothing especially hard here, usual function-minimization proof. 
Take the first derivative, solve it relative to zero, show that this is minimizing solution.
\paragraph{Mathematical notation}
First let's show that $y = a$ is a solution for:
 $$\frac{\partial C}{\partial a} = 0$$
 $$\frac{\partial C}{\partial a} = -\frac{1}{n}\sum(\frac{y}{a} - \frac{1-y}{1-a}) = 0$$
Let's drop $-\frac{1}{n}$, and concetrate on the expression under sum:
 $$\frac{y}{a} - \frac{1-y}{1-a} = 0$$
 $$\frac{y}{a} = \frac{1-y}{1-a}$$
 $$y(1-a) = (1-y)a$$
$$a = y$$
As neurons in the final layer are not intersecting, this holds true for all $j$
Now let's show that the $\frac{\partial^2 C}{\partial^2 a} > 0$ for $y = a$, to prove that we have found minimum:
$$\frac{\partial^2 C}{\partial^2 a} = -\frac{1}{n}\sum(-\frac{y}{a^2} - \frac{1-y}{(1-a)^2})$$
$$-\frac{1}{n}\sum(-\frac{y}{a^2} - \frac{1-y}{(1-a)^2}) > 0$$
substitue $y = a$
$$-\frac{1}{n}\sum(-\frac{y}{y^2} - \frac{1-y}{(1-y)^2}) > 0$$
$$-\frac{1}{n}\sum(-\frac{1}{y} - \frac{1}{1-y}) > 0$$
$$\frac{1}{n}\sum(\frac{1}{y} + \frac{1}{1-y}) > 0$$
Given condition $ 0 <= y <= 1$, it's prettry obvious this inequality holds.
So we have shown that $y = a$ is an extreme value, and that second derivative is larger than zero.
This is sufficient to say that $y = a$ is a minimum. 
Q.E.D.

\section{Learning saturation problems}
\subsection{Saturation for quadratic cost function and sigmoid neurons}
\paragraph{Intuition}
Nothing interesting here, just direct derivation using chain rules
\paragraph{Mathematical notation}
we want to show that:
$$\frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x a^{L-1}_k  (a^L_j-y_j) \sigma'(z^L_j)$$
when $C$ is quadratic loss function.
$$C = \frac{1}{n} \sum_x \sum_j \frac{(y_j-a_j)^2}{2}$$
Applying chain rule (I'm going to need an abbreviature for that),
$$ \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x (y_j - a^L_j) \sigma'(z^L_j) \frac{\partial z}{\partial w^L_{jk}}$$
$$ \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x (y_j - a^L_j) \sigma'(z^L_j) a^{L-1}_{k}$$
Note that we can nullify all other $j$ only if there is no $w^L_{jk}$ in other neurons, which is our case here.
That wasn't hard, was it?)


\subsection{Show error for cross entropy}
\paragraph{Intuition}
Our only hope (Obi-wan Kenobi), is that $ln$-s shall produce smth to cancel the activation value denominator
\paragraph{Mathematical notation}
Here is our cost function:
$$C = -\frac{1}{n} \sum_x \sum_j y_j \ln a^L_j + (1-y_j) \ln (1-a^L_j)$$
Let's take our derivative:
$$\frac{\partial C}{\partial z_j} = -\frac{1}{n} \sum_x \frac{y_j}{a^L_j} \frac{\partial a^L_j}{\partial z}- \frac{1-y_j}{1-a^L_j} \frac{\partial a^L_j}{\partial z}$$
Now let's remember that $\sigma'(z) = \sigma(z)(1 - \sigma(z))$, and substitue it:
$$\frac{\partial C}{\partial z_j} = -\frac{1}{n} \sum_x \frac{y_j}{a^L_j} a^L_j(1 - a^L_j) - \frac{1-y_j}{1-a^L_j} a^L_j(1 - a^L_j)$$
$$\frac{\partial C}{\partial z_j} = -\frac{1}{n} \sum_x y_j(1 - a^L_j) - (1-y_j)a^L_j$$
$$\frac{\partial C}{\partial z_j} = \frac{1}{n} \sum_x a^L_j - y_j$$

\subsection{Show weight derivative for cross entropy}
\paragraph{Intuition}
A usual derivative, just don't forget the $\sigma'(z) = \sigma(z)(1 - \sigma(z))$ trick.
\paragraph{Mathematical notation}
Let's take our previous derivative, but remember that there is now $\frac{\partial z}{\partial w_{jk}}$ new link in a chain:
$$\frac{\partial C}{\partial w_{jk}} = -\frac{1}{n} \sum_x \frac{y_j}{a^L_j} \frac{\partial a^L_j}{\partial z} \frac{\partial z}{\partial w_{jk}} - \frac{1-y_j}{1-a^L_j} \frac{\partial a^L_j}{\partial z} \frac{\partial z}{\partial w_{jk}}$$
Using $\sigma'(z) = \sigma(z)(1 - \sigma(z))$ trick we get:
$$\frac{\partial C}{\partial w_{jk}} = -\frac{1}{n} \sum_x y_j(1 - a^L_j) \frac{\partial z}{\partial w_{jk}} - (1-y_j)a^L_j \frac{\partial z}{\partial w_{jk}}$$
$$\frac{\partial C}{\partial w_{jk}} = \frac{1}{n} \sum_x (a^L_j - y_j) \frac{\partial z}{\partial w_{jk}}$$
As $z_j = \sum_k w_{jk} a^{L-1}_k + b_j$, $\frac{\partial z}{\partial w_{jk}} = a^{L-1}_{k}$:
$$\frac{\partial C}{\partial w_{jk}} = \frac{1}{n} \sum_x (a^L_j - y_j) a^{L-1}_{k}$$
Q.E.D.

A note: for $\frac{\partial C}{\partial b_{j}}$ the process is the same

\subsection{Quadratic cost and linear neurons in output}
Now our cost function for one example is $C = \frac{(y-a^L)^2}{2}$
\paragraph{Intuition}
Nothing interesting here, except a note that we are talking about ouput layer, so learning slowdown may occur for hidden layers.
\paragraph{Mathematical notation}
Error for quaratic fucntion, given that $a^L = z^L$
$$C = \frac{(y-a^L)^2}{2}$$
$$ \frac{\partial C}{\partial z} = \delta_L = y - a^L$$
Derivative for weights:
$$ \frac{\partial C}{\partial w_{jk}} = \frac{1}{n} \sum_x \frac{\partial C}{\partial z_j} \frac{\partial z_j}{\partial w_{jk}}$$
using our previous result for $ \frac{\partial C}{\partial z}$, and taking derivative from $z_j$
$$ \frac{\partial C}{\partial w_{jk}} = \frac{1}{n} \sum_x (y_j - a^L_j) a^{L-1}_k$$
Derivative for biases:
$$ \frac{\partial C}{\partial b_{j}} = \frac{1}{n} \sum_x \frac{\partial C}{\partial z_j} \frac{\partial z_j}{\partial b_{j}}$$
$$ \frac{\partial C}{\partial w_{jk}} = \frac{1}{n} \sum_x (y_j - a^L_j)$$
Q.E.D.

\subsection{Discussion on exclusion of $x_j$ (previous neuron value) from parameter derivaties}
\paragraph{Intuition}
The problem of $x_j$ exclusion (or $a^{l-1}$ for layer $a^l$) is equivalent to the question:
Can we use previous layer in our current layer, in such a way as to not see previous layer in the cost derivatives?
The answer seems to be \'unlikely\', as you can't both train network, and not use changes in previous layers


\end{document}