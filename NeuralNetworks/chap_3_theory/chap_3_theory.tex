\documentclass{article}
\usepackage{amsmath}

\title{Chapter theory 3 solution}
\date{2016-09-27}
\author{Artem Puzanov}


\begin{document}

\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}

A note: this chapter is gargantuan in size, so A LOT of exercises and problems. Not my fault:)

\section{activation derivative proof}
\paragraph{Intuition}
Taking derivatives from exponents rather often ends in a kind of semi-recursive definitions, which come in handy for computational purposes.
\paragraph{Mathematical notation}
We want to prove that:
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
where $\sigma(z) = \frac{1}{1+e^{-z}}$
This is a rather simple derivation, using chain rule:
$$\sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2}$$
Let's take out $\sigma(z)$
$$\sigma'(z) = \frac{1}{(1 + e^{-z})}\frac{e^{-z}}{1+e^{-z}}$$
$$\sigma'(z) = \sigma(z)\frac{e^{-z} + 1 - 1}{1+e^{-z}}$$
$$\sigma'(z) = \sigma(z)(1 - \frac{1}{1+e^{-z}})$$
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
Q.E.D.

\section{entropy function exercies}
\subsection{entropy form memorization}
\paragraph{Intuition}
It's easy to remember specific form, if you follow two rules: it's symmetrical, and logariphm is not defined at zero.
Symmetricity gives: $xln(y) + (1-x)ln(1-y)$, and whether $x$ or $y$ is $a$ or $y$ is easy to decide, if we remember that
true value can take values in $[0,1]$ interval, and activation can't reach $0$ or $1$. 
This means that only activation value can be under $ln$.
No mathematical proof needed here.
\subsection{proof of entropy minimization for regression-like problems}
\paragraph{Intuition}
Nothing especially hard here, usual function-minimization proof. 
Take the first derivative, solve it relative to zero, show that this is minimizing solution.
\paragraph{Mathematical notation}
First let's show that $y = a$ is a solution for:
 $$\frac{\partial C}{\partial a} = 0$$
 $$\frac{\partial C}{\partial a} = -\frac{1}{n}\sum(\frac{y}{a} - \frac{1-y}{1-a}) = 0$$
Let's drop $-\frac{1}{n}$, and concetrate on the expression under sum:
 $$\frac{y}{a} - \frac{1-y}{1-a} = 0$$
 $$\frac{y}{a} = \frac{1-y}{1-a}$$
 $$y(1-a) = (1-y)a$$
$$a = y$$
As neurons in the final layer are not intersecting, this holds true for all $j$
Now let's show that the $\frac{\partial^2 C}{\partial^2 a} > 0$ for $y = a$, to prove that we have found minimum:
$$\frac{\partial^2 C}{\partial^2 a} = -\frac{1}{n}\sum(-\frac{y}{a^2} - \frac{1-y}{(1-a)^2})$$
$$-\frac{1}{n}\sum(-\frac{y}{a^2} - \frac{1-y}{(1-a)^2}) > 0$$
substitue $y = a$
$$-\frac{1}{n}\sum(-\frac{y}{y^2} - \frac{1-y}{(1-y)^2}) > 0$$
$$-\frac{1}{n}\sum(-\frac{1}{y} - \frac{1}{1-y}) > 0$$
$$\frac{1}{n}\sum(\frac{1}{y} + \frac{1}{1-y}) > 0$$
Given condition $ 0 <= y <= 1$, it's prettry obvious this inequality holds.
So we have shown that $y = a$ is an extreme value, and that second derivative is larger than zero.
This is sufficient to say that $y = a$ is a minimum. 
Q.E.D.

\section{Learning saturation problems}
\subsection{Saturation for quadratic cost function and sigmoid neurons}
\paragraph{Intuition}
Nothing interesting here, just direct derivation using chain rules
\paragraph{Mathematical notation}
we want to show that:
$$\frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x a^{L-1}_k  (a^L_j-y_j) \sigma'(z^L_j)$$
when $C$ is quadratic loss function.
$$C = \frac{1}{n} \sum_x \sum_j \frac{(y_j-a_j)^2}{2}$$
Applying chain rule (I'm going to need an abbreviature for that),
$$ \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x (y_j - a^L_j) \sigma'(z^L_j) \frac{\partial z}{\partial w^L_{jk}}$$
$$ \frac{\partial C}{\partial w^L_{jk}} = \frac{1}{n} \sum_x (y_j - a^L_j) \sigma'(z^L_j) a^{L-1}_{k}$$
Note that we can nullify all other $j$ only if there is no $w^L_{jk}$ in other neurons, which is our case here.
That wasn't hard, was it?)


\subsection{Show error for cross entropy}
\paragraph{Intuition}
Our only hope (Obi-wan Kenobi), is that $ln$-s shall produce smth to cancel the activation value denominator
\paragraph{Mathematical notation}
Here is our cost function:
$$C = -\frac{1}{n} \sum_x \sum_j y_j \ln a^L_j + (1-y_j) \ln (1-a^L_j)$$
Let's take our derivative:
$$\frac{\partial C}{\partial z_j} = -\frac{1}{n} \sum_x \frac{y_j}{a^L_j} \frac{\partial a^L_j}{\partial z}- \frac{1-y_j}{1-a^L_j} \frac{\partial a^L_j}{\partial z}$$
Now let's remember that $\sigma'(z) = \sigma(z)(1 - \sigma(z))$, and substitue it:
$$\frac{\partial C}{\partial z_j} = -\frac{1}{n} \sum_x \frac{y_j}{a^L_j} a^L_j(1 - a^L_j) - \frac{1-y_j}{1-a^L_j} a^L_j(1 - a^L_j)$$
$$\frac{\partial C}{\partial z_j} = -\frac{1}{n} \sum_x y_j(1 - a^L_j) - (1-y_j)a^L_j$$
$$\frac{\partial C}{\partial z_j} = \frac{1}{n} \sum_x a^L_j - y_j$$

\subsection{Show weight derivative for cross entropy}
\paragraph{Intuition}
A usual derivative, just don't forget the $\sigma'(z) = \sigma(z)(1 - \sigma(z))$ trick.
\paragraph{Mathematical notation}
Let's take our previous derivative, but remember that there is now $\frac{\partial z}{\partial w_{jk}}$ new link in a chain:
$$\frac{\partial C}{\partial w_{jk}} = -\frac{1}{n} \sum_x \frac{y_j}{a^L_j} \frac{\partial a^L_j}{\partial z} \frac{\partial z}{\partial w_{jk}} - \frac{1-y_j}{1-a^L_j} \frac{\partial a^L_j}{\partial z} \frac{\partial z}{\partial w_{jk}}$$
Using $\sigma'(z) = \sigma(z)(1 - \sigma(z))$ trick we get:
$$\frac{\partial C}{\partial w_{jk}} = -\frac{1}{n} \sum_x y_j(1 - a^L_j) \frac{\partial z}{\partial w_{jk}} - (1-y_j)a^L_j \frac{\partial z}{\partial w_{jk}}$$
$$\frac{\partial C}{\partial w_{jk}} = \frac{1}{n} \sum_x (a^L_j - y_j) \frac{\partial z}{\partial w_{jk}}$$
As $z_j = \sum_k w_{jk} a^{L-1}_k + b_j$, $\frac{\partial z}{\partial w_{jk}} = a^{L-1}_{k}$:
$$\frac{\partial C}{\partial w_{jk}} = \frac{1}{n} \sum_x (a^L_j - y_j) a^{L-1}_{k}$$
Q.E.D.

A note: for $\frac{\partial C}{\partial b_{j}}$ the process is the same

\subsection{Quadratic cost and linear neurons in output}
Now our cost function for one example is $C = \frac{(y-a^L)^2}{2}$
\paragraph{Intuition}
Nothing interesting here, except a note that we are talking about ouput layer, so learning slowdown may occur for hidden layers.
\paragraph{Mathematical notation}
Error for quaratic fucntion, given that $a^L = z^L$
$$C = \frac{(y-a^L)^2}{2}$$
$$ \frac{\partial C}{\partial z} = \delta_L = y - a^L$$
Derivative for weights:
$$ \frac{\partial C}{\partial w_{jk}} = \frac{1}{n} \sum_x \frac{\partial C}{\partial z_j} \frac{\partial z_j}{\partial w_{jk}}$$
using our previous result for $ \frac{\partial C}{\partial z}$, and taking derivative from $z_j$
$$ \frac{\partial C}{\partial w_{jk}} = \frac{1}{n} \sum_x (y_j - a^L_j) a^{L-1}_k$$
Derivative for biases:
$$ \frac{\partial C}{\partial b_{j}} = \frac{1}{n} \sum_x \frac{\partial C}{\partial z_j} \frac{\partial z_j}{\partial b_{j}}$$
$$ \frac{\partial C}{\partial w_{jk}} = \frac{1}{n} \sum_x (y_j - a^L_j)$$
Q.E.D.

\subsection{Discussion on exclusion of $x_j$ (previous neuron value) from parameter derivaties}
\paragraph{Intuition}
The problem of $x_j$ exclusion (or $a^{l-1}$ for layer $a^l$) is equivalent to the question:
Can we train a network when neuron in the next layer either doesn't depend on the previous layer, or depends on it in such a way that
$\frac{\partial C}{\partial w_{jk}}$ has no dependecy on the previous layer? 
The answer is: we can, but there shall be no weigths parameter, only $b_j$. 
The explanation is that to have no dependency on $x_j$, it must enter the activation funtion in a way that $\frac{\partial C}{\partial w_{jk}}$ should give it as a constant in some form. In order for it to be a constant, it has to be separated from the $w_{jk}$ in the activation. Without it, weigths are just an array of constants, so we can group them into one consant, hence $b_j$. 
A note: such a network can still be trained! - backprogation equations still make sense here. It's just the parameters for optimization shall include only constants on neurons. It shall still be capable of reacting to different inputs differently, but each neuron shall not have the option of reweighting each previous neuron individually.

No mathematical notation here, maybe later.

\section{Softmax}

\subsection{Example of not $1$ sum for usual network}
\paragraph{Intuition}
Proving such statement requires no math whatsoever - there is literally nothing stopping us from assigning
weights and biases in any way we like, in any neural network. The better question is this:
Can a network to some sensible task generate an output layer, where activation valus shall not sum up to $1$?
The answer is again yes: let's rememeber binary encoding for numbers classification - each of the correct answers was
either one or two neurons activated. For the numbers that were encoded by two activated bits, the trained network would ouput 
sum of ouput neurons larger than 1.

Again, no need for mathematical notation here.

\subsection{Monothonity of softmax}
\paragraph{Intuition}
Increase in numerator is in terms of percents always larger than increase in denominator for each positive $\delta z_j$.
So first derivatve should be positive. 
$z_{k, k \neq j}$ exists only in denomiator, so it's increase should lead directly to decrease of $a$
\paragraph{Mathematical notation}
$$a^L_j = \frac{e^{z^L_j}}{\sum_{k} e^{z^L_k}}$$
First let's work on differenttation by $z^L_j$.
Using differentiation rules for fractions:
$$\frac{\partial a^L_j}{\partial z^L_j} = \frac{e^{z^L_j} \sum_{k} e^{z^L_k} - e^{z^L_j} e^{z^L_j}}{(\sum_{k} e^{z^L_k})^2}$$
$$\frac{\partial a^L_j}{\partial z^L_j} = \frac{e^{z^L_j} \sum_{k} e^{z^L_k} - e^{z^L_j} e^{z^L_j}}{(\sum_{k} e^{z^L_k})^2}$$
$$\frac{\partial a^L_j}{\partial z^L_j} = \frac{e^{z^L_j} \sum_{k \neq j} e^{z^L_k}}{(\sum_{k} e^{z^L_k})^2}$$
Exponent is always positive, so $\frac{\partial a^L_j}{\partial z^L_j} > 0$
Now let's work on differenttation by $z^L_k$:
$$\frac{\partial a^L_j}{\partial z^L_k} = -\frac{e^{z^L_k} e^{z^L_j}}{(\sum_{k} e^{z^L_k})^2}$$
The denominator is positive, the numerator is positive, so the whole thingy is negative
Q.E.D.

\subsection{Inverting the softmax layer}
\paragraph{Intuition}
Nothing interesting here.
\paragraph{Mathematical notaion}
$$a^L_j = \frac{e^{z^L_j}}{\sum_{k} e^{z^L_k}}$$
$$ln(a^L_j) = ln(e^{z^L_j}) - ln(\sum_{k}e^{z^L_k})$$
$$ln(a^L_j) = z^L_j - ln(\sum_{k}e^{z^L_k})$$
As we can see, $ln(\sum_{k}e^{z^L_k})$ is the same for all neurons, so we can replace it with $C$.
$$z^L_j = ln(a^L_j) + C$$
Q.E.D.

\subsection{Differentiation of parameters for Loglikelihood}
\paragraph{Intuition}
Again, nothing particulary interesting, except a way to include $y_j = 1$, but that works out rather naturally.
\paragraph{Mathematical notation}
$$C \equiv - ln(a^L_y)$$
Let's strart with weight $w_{jk}$
$$\frac{\partial C}{\partial w^L_{jk}} = - \frac{\partial C}{\partial a^L_{j}} \frac{\partial a^L_{j}}{\partial z^L_j}\frac{\partial z^L_j}{w_{jk}}$$
From the previous exercise we know $\frac{\partial a^L_j}{\partial z^L_j}$, so let's substitute it (along with other stuff):
$$\frac{\partial C}{\partial w^L_{jk}} = - \frac{1}{a^L_j}  \frac{e^{z^L_j} \sum_{k \neq j} e^{z^L_k}}{(\sum_{k} e^{z^L_k})^2}  a^{L-1}_k$$
$$\frac{\partial C}{\partial w^L_{jk}} = - 1 \frac{\sum_{k \neq j} e^{z^L_k} + e^{z^L_j} - e^{z^L_j}}{ (\sum_{k} e^{z^L_k}) }  a^{L-1}_k$$
$$\frac{\partial C}{\partial w^L_{jk}} = - 1(1 - \frac{e^{z^L_j}}{ \sum_{k} e^{z^L_k}}) a^{L-1}_k$$
That fraction is acutally activation for $j$
$$\frac{\partial C}{\partial w^L_{jk}} = (a^{L}_j - 1) a^{L-1}_k$$.
(Strange stuff here, recheck:)
for $j$, $y_j = 1$, so we can substitue that to get:
$$\frac{\partial C}{\partial w^L_{jk}} = (a^{L}_j - y_j) a^{L-1}_k$$.

For $b_j$:
$$\frac{\partial C}{\partial b^L_{j}} = - \frac{\partial C}{\partial a^L_{j}} \frac{\partial a^L_{j}}{\partial z^L_j}\frac{\partial z^L_j}{b_{j}}$$
The solution is the same as for $w_{jk}$, except that $\frac{\partial z^L_j}{\partial b^L_{j}} = 1$, so there is no $a^{L-1}_k$ part.
$$\frac{\partial C}{\partial b^L_{j}} = a^{L}_j - y_j$$.

Q.E.D.

\subsection{Softmax origination and naming reasons}
\paragraph{Intuition}
About probability distribution: we have to show that for every event probability/density is non-negative,
and that probabilities of all possible events sum up to 1.

About limiting value of activations: as we have the same value in the numerator and as part of the denomiator,
it's pretty easy to see that this fraction is strictly non-negative, and can never be larger than 1.

The behavior of function for differnt $c$ is not obivous, so we will have to check differentiantions.

Derivation of $\frac{\partial C}{\partial z_j}$ is straighforward, we kinda did before actually, for $b_j$ and $w_j$.

\paragraph{Mathematical notation}
Let's show that all activations sum up to $1$. We can take equation (79) for the texbook itself, and add $c$ to it:
$$\sum_j a^L_j  = \sum_j \frac{ e^{C z^L_j}}{\sum_k e^{C z^L_k}} = 1$$
Non negativity follows directly from usage of $e$, as it is alwasys non-negative.
These two conditions are sufficient (not very strict sense though) to use this \'flexible\' softmax as a distribution function.

Let's differentiate softmax by $c$:
$$a^L_j = \frac{e^{cz^L_j}}{\sum_{k} e^{cz^L_k}}$$
$$\frac {\partial a^L_j}{\partial c} = \frac {z^L_j e^{cz^L_j} \sum_k e^{cz^L_k} - e^{cz^L_j} \sum_k z^L_k e^{cz^L_k}}{(\sum_{k} e^{cz^L_k})^2}$$
$$\frac {\partial a^L_j}{\partial c} = e^{cz^L_j} \frac {z^L_j \sum_k e^{cz^L_k} - \sum_k z^L_k e^{cz^L_k}}{(\sum_{k} e^{cz^L_k})^2}$$
$$\frac {\partial a^L_j}{\partial c} = e^{cz^L_j} \frac {\sum_k (z^L_j - z^L_k) e^{cz^L_k}}{(\sum_{k} e^{cz^L_k})^2}$$
Two facts are very easy to see now:
$$\forall_k z_j^L > z_k^L: \frac {\partial a^L_j}{\partial c} > 0$$
$$\forall_k z_j^L < z_k^L: \frac {\partial a^L_j}{\partial c} < 0$$
So, if our neuron is stronger than all others, $c$ makes it (relatively) more powerful, and if it is weaker, $c$ makes it even weaker.
In terms of function form, this means that the larger $c$ the steeper is shift from $0$ to $1$ as $z_j$ increases.

And finally, error derivation:
$$\frac{\partial C}{\partial z^L_{j}} = - \frac{\partial C}{\partial a^L_{j}} \frac{\partial a^L_{j}}{\partial z^L_j}$$
Using approach from previous exercises:
$$\frac{\partial C}{\partial z^L_{j}} = - \frac{1}{a^L_j}  \frac{e^{z^L_j} \sum_{k \neq j} e^{z^L_k}}{(\sum_{k} e^{z^L_k})^2}$$
$$\frac{\partial C}{\partial z^L_{j}} = - 1 \frac{\sum_{k \neq j} e^{z^L_k} + e^{z^L_j} - e^{z^L_j}}{ (\sum_{k} e^{z^L_k}) }$$
$$\frac{\partial C}{\partial z^L_{j}} = - 1(1 - \frac{e^{z^L_j}}{ \sum_{k} e^{z^L_k}})$$
$$\frac{\partial C}{\partial z^L_{j}} = (a^{L}_j - 1)$$.
(Strange stuff here, recheck:)
for $j$, $y_j = 1$, so we can substitue that to get:
$$\frac{\partial C}{\partial z^L_{j}} = a^{L}_j - y_j$$.

Q.E.D.

\section{Artificial dataset expansion}
The problem with arbitrarily large rotations is actually two-fold:
1) We make training examples look like \'other\' training examples. $6$ and $9$ for example shall generate a lot of fun.
To make that work, we shall have to use even larger set, which kinda defeats the point.
2) Overall teaching the model extremly-rare real-life cases seems like a good way
to waste a lot of optimization time on very small part of real world problem. It's much easier to first classify whether the dataset has been reversed, and than classify digits.

\section{Asymptotic efficiency of algorithms}
This is a large topic/question, we'll return here later. Some sources:

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3307431/

\section{Initialization tricks}
\subsection{Standart deviation estimation}
\paragraph{Intuition}
We are dealing with independent random variables, so this is pretty easy.

\paragraph{Mathematical notation}
$$Var(z) = \sum_j{Var(w_j) x_j^2} + Var(b)$$
$$Var(z) = \sum_j{\frac{1}{n_{in}}x_j^2} + 1$$
Now $n_in = 1000$, and $500$ of $1000 x_j$-s is $1$, so:
$$Var(z) = \frac{1}{1000}500 + 1$$
$$Var(z) = \frac{3}{2}$$
$$\sigma(z) = \sqrt{\frac{3}{2}}$$

Tada.

\section{Regularization heuristscs}
\subsection{First epochs dominated by weight decay}

CHECK THIS AGAIN

\paragraph{Intuition}
Using the old weights initialization gives a very fat-tailed distribution. 
This means large $w$ are very probable. Given that $w$ is present in weights update
directly only in \'weight decay'\ part, the larger the weight, the faster is decay. 
The cost function may be exotic enough to have large derivative too, but there is no reason to do so.
\paragraph{Mathematical notation}
To see how first epochs are dominated, let's consider the $\frac {\partial C_0}{\partial w_{jk}}$ part of the weight update.
If we assume cross-entropy loss function and sigmoid neurons, than
$\frac {\partial C_0}{\partial w_{jk}} = a^{L-1}_k (a^L_j - y_j)$
Now, we assume that due to old weights initialization, activations are either $0$ or $1$, with equal probabilities.
Also assume that $y_j = 1$ with probability of $0.1$.
It's easy to see that half the time this value is $0$, as $a^{L-1}_k = 0$.
If $a^{L-1}_k \neq 0$, than here are other outcomes and their corresponding probabilities

$$a^{L-1}_k (a^L_j - y_j) = 1(0-0):  0.5 * 0.5 * 0.9$$
$$a^{L-1}_k (a^L_j - y_j) = 1(1-0):  0.5 * 0.5 * 0.9$$
$$a^{L-1}_k (a^L_j - y_j) = 1(0-1):  0.5 * 0.5 * 0.1$$
$$a^{L-1}_k (a^L_j - y_j) = 1(1-1):  0.5 * 0.5 * 0.1$$

This means that in $0.75$ of the cases, value of $\frac {\partial C_0}{\partial w_{jk}} = 0$.
In $0.25$ of the cases,  $\frac {\partial C_0}{\partial w_{jk}} = 1$ or $\frac {\partial C_0}{\partial w_{jk}} = -1$.
Of those $1$ has $0.9$ probability, $-1$ has $0.1$ probability. So expectation is $0.25*((0.9) * 1 + 0.1 * (-1)) = 0.25 * 0.8 = 0.2$.
This means that if the choice is random and guessing correct answer is likely to give more wrong answers than right, 
we are trying to decrease every weigth.

Another thing to consier is that $a^{L-1}_k (a^L_j - y_j)$ has a maximum, and $w$ (which is random), does not.
Hence choice of $\lambda$ allows $|w|$ to excced $| \frac {\partial C_0}{\partial w_{jk}} |$ with any chosen probability.

\subsection{Speed of decay and tailing out}
These problems shall be solved later (current solutions ideas are rather bad)

\end{document}
