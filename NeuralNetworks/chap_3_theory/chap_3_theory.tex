\documentclass{article}
\usepackage{amsmath}

\title{Chapter theory 3 solution}
\date{2016-09-27}
\author{Artem Puzanov}


\begin{document}

\maketitle
\pagenumbering{gobble}
\newpage
\pagenumbering{arabic}

\section{activation derivative proof}
\paragraph{Intuition}
Taking derivatives from exponents rather often ends in a kind of semi-recursive definitions, which come in handy for computational purposes.
\paragraph{Mathematical notation}
We want to prove that:
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
where $\sigma(z) = \frac{1}{1+e^{-z}}$
This is a rather simple derivation, using chain rule:
$$\sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2}$$
Let's take out $\sigma(z)$
$$\sigma'(z) = \frac{1}{(1 + e^{-z})}\frac{e^{-z}}{1+e^{-z}}$$
$$\sigma'(z) = \sigma(z)\frac{e^{-z} + 1 - 1}{1+e^{-z}}$$
$$\sigma'(z) = \sigma(z)(1 - \frac{1}{1+e^{-z}})$$
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$
Q.E.D.

\section{entropy function exercies}
\subsection{entropy form memorization}
\paragraph{Intuition}
It's easy to remember specific form, if you follow two rules: it's symmetrical, and logariphm is not defined at zero.
Symmetricity gives: $xln(y) + (1-x)ln(1-y)$, and whether $x$ or $y$ is $a$ or $y$ is easy to decide, if we remember that
true value can take values in $[0,1]$ interval, and activation can't reach $0$ or $1$. 
This means that only activation value can be under $ln$.
No mathematical proof needed here.
\subsection{proof of entropy minimization for regression-like problems}
\paragraph{Intuition}
Nothing especially hard here, usual function-minimization proof. 
Take the first derivative, solve it relative to zero, show that this is minimizing solution.
\paragraph{Mathematical notation}
First let's show that $y = a$ is a solution for:
 $$\frac{\partial C}{\partial a} = 0$$
 $$\frac{\partial C}{\partial a} = -\frac{1}{n}\sum(\frac{y}{a} - \frac{1-y}{1-a}) = 0$$
Let's drop $-\frac{1}{n}$, and concetrate on the expression under sum:
 $$\frac{y}{a} - \frac{1-y}{1-a} = 0$$
 $$\frac{y}{a} = \frac{1-y}{1-a}$$
 $$y(1-a) = (1-y)a$$
$$a = y$$
As neurons in the final layer are not intersecting, this holds true for all $j$
Now let's show that the $\frac{\partial^2 C}{\partial^2 a} > 0$ for $y = a$, to prove that we have found minimum:
$$\frac{\partial^2 C}{\partial^2 a} = -\frac{1}{n}\sum(-\frac{y}{a^2} - \frac{1-y}{(1-a)^2})$$
$$-\frac{1}{n}\sum(-\frac{y}{a^2} - \frac{1-y}{(1-a)^2}) > 0$$
substitue $y = a$
$$-\frac{1}{n}\sum(-\frac{y}{y^2} - \frac{1-y}{(1-y)^2}) > 0$$
$$-\frac{1}{n}\sum(-\frac{1}{y} - \frac{1}{1-y}) > 0$$
$$\frac{1}{n}\sum(\frac{1}{y} + \frac{1}{1-y}) > 0$$
Given condition $ 0 <= y <= 1$, it's prettry obvious this inequality holds.
So we have shown that $y = a$ is an extreme value, and that second derivative is larger than zero.
This is sufficient to say that $y = a$ is a minimum. 
Q.E.D.

\section{Learning saturation problems}
\subsection{Saturation for quadratic cost function and sigmoid neurons}

\end{document}